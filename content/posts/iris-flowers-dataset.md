---
title: Iris Flowers Dataset with Keras
date: 2019-11-01T21:00:00+08:00
published: true
slug: iris-keras
tags:
- Keras
- iris
cover_image: "./images/iris-keras.png"
canonical_url: false
description: Build and use neural networks for the Iris classification task via Keras.
---

:::note ğŸ“• Motto

â€œç°ä»Šç¨‹åºå‘˜çš„æƒ…å†µå¥½å¤šäº†ï¼Œåªè¦æœ‰ä¸€å°ä¾¿å®œçš„äºŒæ‰‹ç”µè„‘ï¼Œä¸€å¼ Linuxå…‰ç›˜å’Œä¸€ä¸ªäº’è”ç½‘å¸æˆ·ï¼Œä½ å°±å·²ç»æ‹¥æœ‰äº†æŠŠè‡ªå·±æå‡åˆ°ä»»ä½•çº§åˆ«çš„ç¼–ç¨‹æ°´å¹³æ‰€éœ€çš„å…¨éƒ¨å·¥å…·ã€‚â€
â€œåœ¨ä¿¡æ¯æ—¶ä»£ï¼Œè¿›å…¥ç¼–ç¨‹é¢†åŸŸçš„å£å’å®Œå…¨ä¸å­˜åœ¨äº†ã€‚å³ä½¿æœ‰ä¹Ÿæ˜¯è‡ªæˆ‘å¼ºåŠ çš„ã€‚å¦‚æœä½ æƒ³ç€æ‰‹å»å¼€å‘ä¸€äº›å…¨æ–°çš„ä¸œè¥¿ï¼Œä½ ä¸éœ€è¦æ•°ç™¾ä¸‡ç¾å…ƒçš„èµ„æœ¬ã€‚ä½ åªéœ€è¦è¶³å¤Ÿçš„æ¯”è¨å’Œå¥æ€¡å¯ä¹å­˜åœ¨ä½ çš„å†°ç®±é‡Œï¼Œæœ‰ä¸€å°ä¾¿å®œçš„PCç”¨äºå·¥ä½œï¼Œä»¥åŠè®©ä½ åšæŒä¸‹æ¥çš„å¥‰çŒ®ç²¾ç¥ã€‚**æˆ‘ä»¬ç¡åœ¨åœ°æ¿ä¸Šã€‚æˆ‘ä»¬è·‹å±±æ¶‰æ°´**ã€‚â€

â€”â€” çº¦ç¿°Â·å¡é©¬å…‹
:::

![](https://i.loli.net/2021/01/07/XR892ceDVO5h1fZ.png)

çº¦ç¿°Â·å¡é©¬å…‹çš„æœ€åä¸€å¥è¯ï¼Œé€šä¿—æ˜“æ‡‚ã€‚åˆä¸ç¦è®©æˆ‘æƒ³èµ·äº†é‚£æ®µå°è¯ï¼š

> è¿™æ˜¯æœ€å¥½çš„æ—¶ä»£ï¼Œè¿™æ˜¯æœ€åçš„æ—¶ä»£ã€‚æˆ‘ä»¬ä¸€æ— æ‰€æœ‰ï¼Œæˆ‘ä»¬å·ç„¶çŸ—ç«‹ã€‚

:::note â„¹ï¸ Introduction

We will build and use a neural network for the Iris classification task. We will use Keras as a high-level library for managing neural networks.

:::

<!-- more -->

## Analysis Code

```python
import numpy as np

import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris  # import load_iris function from datasets module

from keras.models import Sequential, Input, Model
from keras.utils import to_categorical
from keras.layers import Dense, Dropout, Flatten
from keras import optimizers

# -- Load Iris dataset using sklearn (save "bunch" object containing iris dataset and its attributes) -- ##
# å¯¼å…¥Irisæ•°æ®é›†
# Irisæ•°æ®é›†æ˜¯sklearnä¸­è‡ªå¸¦çš„
# Irisæ•°æ®é›†æœ‰3ä¸ªç§ç±»ï¼Œ4ä¸ªå±æ€§ï¼Œæ¯ä¸ªå±æ€§æœ‰50ä¸ªæ ·ä¾‹
iris = load_iris()
# xä»£è¡¨irisæ•°æ®é›†çš„æ•°æ®
X = iris.data
# yä»£è¡¨ç€ç€irisçš„ç›®æ ‡å±æ€§ï¼Œå³èŠ±çš„ç±»å‹
y = iris.target

# -- Change the labels from categorical to one-hot encoding -- ##
# The output neurons of the NN will be trained to match the one_hot encoded array
# Example: category label 0 out of 3 labels becomes [1 0 0];
# Example: category label 1 out of 3 labels becomes [0 1 0];
# HINT: use: "to_categorical" to redifine the one hot encoded target y_one_hot using the original y.
# for i in range(len(y)):
#     if y[i] == 0:
#         y[i] = 1
#     else:
#         y[i] = 0
y_one_hot = to_categorical(y)
print(X.shape)
print(y_one_hot.shape)
# y_one_hot = np.reshape(y, (-1, 1))

# -- Split the dataset for training, validation, and test -- ##

# xæ˜¯è¢«åˆ’åˆ†çš„æ ·æœ¬ç‰¹å¾é›†
# y_one_hotæ˜¯è¢«åˆ’åˆ†çš„æ ·æœ¬æ ‡ç­¾
# å¦‚æœæ˜¯æµ®ç‚¹æ•°ï¼Œå°±åœ¨0~1ä¹‹é—´ï¼Œè¡¨ç¤ºæ ·æœ¬æ‰€å æ¯”ä¾‹ï¼›å¦‚æœæ˜¯æ•´æ•°ï¼Œå°±æ˜¯æ ·æœ¬çš„æ•°é‡
train_and_valid_X, test_X, train_and_valid_y, test_y = train_test_split(X, y_one_hot, test_size=0.1)
train_X, valid_X, train_y, valid_y = train_test_split(train_and_valid_X, train_and_valid_y, test_size=0.2)


# define the neural network
def baseline_model():
    nb_nurons = 8
    # nb_Classes = 1  # HINT:  <---- look here
    nb_Classes = 3
    input_dimensions = 4
    learning_rate = 0.002
    # create model
    # keras.models.Sequentialæ˜¯ç¥ç»ç½‘ç»œæ¨¡å‹çš„å°è£…å®¹å™¨ã€‚å®ƒä¼šæä¾›å¸¸è§çš„å‡½æ•°
    model = Sequential()
    # ç¬¬ä¸€å±‚çº§ - æ·»åŠ æœ‰ input_dimensions = 4ä¸ªèŠ‚ç‚¹çš„è¾“å…¥å±‚
    # æ¿€æ´»å‡½æ•°ä½¿ç”¨ReLUè¿ç®—
    model.add(Dense(nb_nurons, input_dim=input_dimensions, activation='relu'))
    # HINT: a 'softmax' activation will output a probability distribution over the output dimensions
    # æ¿€æ´»å‡½æ•°ä½¿ç”¨softmaxå‡½æ•°
    model.add(Dense(nb_Classes,
                    activation='softmax'))
    # Compile model
    # å®ä¾‹åŒ–ä¸€ä¸ªä¼˜åŒ–å™¨å¯¹è±¡ï¼Œè¿™é‡Œé‡‡ç”¨RMSpropä¼˜åŒ–å™¨
    opt = optimizers.RMSprop(lr=learning_rate)
    # HINT: a 'binary_crossentropy' is only useful for at most 2 labels, look for another suitable loss function in Keras
    # compileç”¨äºé…ç½®è®­ç»ƒæ¨¡å‹ï¼Œlossæ˜¯å­—ç¬¦ä¸²æˆ–ç›®æ ‡å‡½æ•°åï¼Œoptimizeræ˜¯ä¼˜åŒ–å™¨åæˆ–ä¼˜åŒ–å™¨å®ä¾‹ï¼Œmetricsæ˜¯åœ¨è®­ç»ƒå’Œæµ‹è¯•æœŸé—´çš„æ¨¡å‹è¯„ä¼°æ ‡å‡†
    # binary_crossentropyæ˜¯äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œä¸€èˆ¬ç”¨äºäºŒåˆ†ç±»
    # å› ä¸ºè¿™é‡Œè¦å®ç°3ä¸­åˆ†ç±»å³å¤šåˆ†ç±»ï¼Œæ‰€ä»¥ä½¿ç”¨categorical_crossentropy
    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model


def find_correct_and_incorrect_labels(model, test_X, test_y):
    # Computes for every input in the test dataset a probability distribution over the categories
    # predictæ˜¯ä¸ºè¾“å…¥æ ·æœ¬ç”Ÿæˆè¾“å‡ºé¢„æµ‹ï¼Œè®¡ç®—æ˜¯åˆ†æ‰¹è¿›è¡Œçš„
    predicted_classes = model.predict(test_X)
    # argmaxæ˜¯æ‰¾åˆ°æ ·æœ¬ä»¥æœ€å¤§æ¦‚ç‡æ‰€å±çš„ç±»åˆ«ä½œä¸ºæ ·æœ¬çš„é¢„æµ‹æ ‡ç­¾
    # HINT: choose the prediction with the highest probability, np.argmax( ..... , axis=1 )
    # np.roundæ˜¯å–è¿”å›å››èˆäº”å…¥åçš„å€¼ï¼Œå¯æŒ‡å®šç²¾åº¦ï¼Œä¸np.aroundç­‰æ•ˆ
    predicted_classes = np.argmax(predicted_classes, axis=1)
    # np.where(conditions)æ»¡è¶³conditionsçš„æ¡ä»¶å³è¾“å‡ºæ•°ç»„çš„ä¸‹æ ‡
    correctIndex = np.where(predicted_classes == np.argmax(test_y))[0]  # HINT: replace test_y by np.argmax(test_y,axis=1)
    incorrectIndex = np.where(predicted_classes != np.argmax(test_y))[0]  # HINT: replace test_y by np.argmax(test_y,axis=1)
    print("Found %d correct labels using the model" % len(correctIndex))
    print("Found %d incorrect labels using the model" % len(incorrectIndex))


def plot_train_performance(trained_model):
  	# historyå‡½æ•°ä¼šæ¯è½®è®­ç»ƒæ”¶é›†æŸå¤±å’Œå‡†ç¡®ç‡ï¼Œå¦‚æœæœ‰æµ‹è¯•é›†ä¹Ÿä¼šæ”¶é›†æµ‹è¯•é›†çš„æ•°æ®
    print(trained_model.history.keys())
    accuracy = trained_model.history['accuracy']
    val_accuracy = trained_model.history['val_accuracy']
    loss = trained_model.history['loss']
    val_loss = trained_model.history['val_loss']
    # epochè¡¨ç¤ºå®Œæˆäº†1éè®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ ·æœ¬
    epochs = range(len(accuracy))
    # åˆ›å»ºä¸€ä¸ªç”»æ¿ï¼Œ1ä¸ºç”»æ¿çš„ç¼–å·ï¼Œå¯ä»¥ä¸å¡«
    f1 = plt.figure(1)
    # ä¸€ä¸ªfigureå¯¹è±¡åŒ…å«äº†å¤šä¸ªå­å›¾ï¼Œå¯ä»¥ä½¿ç”¨subplot()å‡½æ•°æ¥ç»˜åˆ¶å­å›¾
    plt.subplot(1, 2, 1)
    # axisè®¾ç½®åæ ‡è½´
    plt.axis((0, len(epochs), 0, 1.2))
    plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
    plt.plot(epochs, val_accuracy, 'b', label='valid accuracy')
    plt.title('Training and valid accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.axis((0, len(epochs), 0, 1.2))
    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='valid loss')
    plt.title('Training and valid loss')
    # å±•ç¤ºå‡ºæ¯ä¸ªæ•°æ®å¯¹åº”çš„å›¾åƒåç§°ï¼Œæ›´å¥½è¾¨è®¤
    plt.legend()
    plt.show()
    plt.pause(0.001)


# -- Initialize model -- ##
model = baseline_model()

# -- Test the performmance of the untrained model over the test dataset -- ##
find_correct_and_incorrect_labels(model, test_X, test_y)

# -- Train the model -- ##
print('\nTraining started ...')
trained_model = model.fit(train_X, train_y, batch_size=10, epochs=150, verbose=0, validation_data=(valid_X, valid_y))
print('Training finished. \n')

# -- Test the performmance of the trained model over the test dataset -- ##
find_correct_and_incorrect_labels(model, test_X, test_y)

# -- Plot performance over training episodes -- ##
plot_train_performance(trained_model)
```

## Result

![](https://i.loli.net/2021/01/07/2flu4zmsqdJ1FLR.png)

## Error

åœ¨æŒ‰ç…§æç¤ºä¿®æ”¹ä»£ç åï¼Œä¼šä¸€ç›´æç¤ºä»¥ä¸‹è¿™ä¸ªé”™è¯¯ï¼Œå³å®é™…è¾“å‡ºçš„æ•°ç»„å½¢çŠ¶ä¸ºä¸é¢„æœŸæ•°æ®çš„å½¢çŠ¶ä¸åŒã€‚

:::warning

`ValueError: Error when checking target: expected dense_2 to have shape (1,) but got array with shape (3,)`

:::

æœ€ç»ˆé€šè¿‡Stack Overflowä¸Šçš„[ä¸€ç¯‡å¸–å­](https://stackoverflow.com/questions/51456613/valueerror-error-when-checking-target-expected-dense-3-to-have-shape-1-but)æ‰¾åˆ°äº†é—®é¢˜æ‰€åœ¨ã€‚

the following line is wrong:

```python
nb_Classes = 1
```

change it to:

```python
nb_Classes = 3
```

Because final layer is of 3 dimension. Beacuse I used `categorical_crossentropy` and also the terminal shows that actually there are three layers.
