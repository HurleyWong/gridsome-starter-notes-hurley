---
title: å¤–æ–‡ç¿»è¯‘ï¼šA Gentle Introduction to Calculating the BELU Score for Text in Python
date: 2020-04-04T21:00:00+08:00
published: false
slug: calculating-belu-for-text
tags:
- BLEU
- translate
cover_image: "./images/calculating-belu-for-text.png"
canonical_url: false
description: å¯¹å¤–æ–‡ "A Gentle Introduction to Calculating the BELU Score for Text in Python" çš„ç¿»è¯‘ä¸è§£é‡Š
---

:::note ğŸ“œ Intro

BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation or text to one or more reference translations.

Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.

:::

### Aim

* A gentle introduction to the BLEU score and an intuition for what is being calculated.
* How to calculate BLEU scores in Python using the NLTK library for sentences and documents.
* How to use a suite of small examples to develop an intuition for how differences between a candidate and reference text impact the final BLEU score.

<!-- more -->

### Bilingual Evaluation Understudy Score

ç®€ç§° BLEUï¼Œæ„æ€æ˜¯åŒè¯­è¯„ä¼°æ›¿è¡¥ï¼Œæ˜¯ç”¨äºè¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„å¥å­ (candidate) å’Œå®é™…çš„å¥å­ (reference) çš„å·®å¼‚çš„æŒ‡æ ‡ã€‚

å®ƒçš„å–å€¼èŒƒå›´åœ¨ $[0, 1]$ ä¹‹é—´ã€‚å¦‚æœä¸¤ä¸ªå¥å­å®Œç¾åŒ¹é… (perfect match)ï¼Œé‚£ä¹ˆ BLEU æ˜¯ 1.0ï¼›å¦‚æœä¸¤ä¸ªå¥å­æ˜¯å®Œç¾ä¸åŒ¹é… (perfect mismatch)ï¼Œé‚£ä¹ˆ BLEU æ˜¯ 0.0ã€‚

BLEU åˆ†æ•°æ˜¯ç”± Kishore Papineni åœ¨å…¶ 2002 å¹´çš„è®ºæ–‡ "*BLEU: a Method for Automatic Evaluation of Machine Translation*" ä¸­æå‡ºã€‚

1. è¯¥æ–¹æ³•é€šè¿‡åˆ†åˆ«è®¡ç®— candidate å¥ä¸­ n-gram ä¸ reference ä¸­çš„ n-gram æ¥å·¥ä½œçš„ã€‚

    > The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matched. These matches are position-independent. The more the matched, the better the candidate translation is.

2. ä½¿ç”¨ä¿®æ­£çš„ n-gram ç²¾åº¦ã€‚å³ä¿®æ”¹åŒ¹é…ä¸­ n-gram çš„æŠ€æœ¯ä»¥ç¡®ä¿å°† reference ä¸­çš„å•è¯çš„å‡ºç°ä¹Ÿè€ƒè™‘åœ¨å†…ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¤§é‡çš„ candidate çš„ç¿»è¯‘ã€‚

    > Unfortunately, MT systems can overgenerate "reasonable" words, resulting in improbable, but high-precision, translations [...] Intuitively the problem is clear: a reference word should be considered exhausted after a matching candidate word is identified. We formalize this intuition as the modified unigram precision.

**unigram** çš„å‡†ç¡®ç‡å¯ä»¥ç”¨äºè¡¡é‡å•è¯ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œæ›´é«˜é˜¶çš„ **n-gram** çš„å‡†ç¡®ç‡å¯ä»¥ç”¨æ¥è¡¡é‡å¥å­çš„æµç•…æ€§ã€‚

> BLEUåå‘äºçŸ­ç¿»è¯‘åˆ†æ•°æ›´é«˜ä¸€ç‚¹ã€‚

### Calculate BLEU Scores

Python è‡ªç„¶è¯­è¨€å·¥å…·åº“æˆ–è€… NLTK æä¾›äº† BLEU åˆ†æ•°çš„è®¡ç®—æ–¹æ³•ï¼Œå¯ä»¥ä½¿ç”¨è¯¥åˆ†æ•°æ¥è¯„ä¼°ç”Ÿæˆçš„æ–‡æœ¬ã€‚

#### å¥å­çš„BLEUå€¼

NLTK æä¾›`sentence_bleu()`æ–¹æ³•æ¥è¯„ä¼°ä¸€ç»„ candidate å’Œ reference çš„ BLEU å¾—åˆ†æƒ…å†µã€‚ä¾‹å¦‚ï¼š

```python
from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'a', 'test'], ['this', 'is', 'test']]
candidate = ['this', 'is', 'a', 'test']
score = sentence_bleu(reference, candidate)
print(score)
```

è¿è¡Œä¸Šé¢è¿™ä¸ªä¾‹å­ä¼šå¾—åˆ°ä¸€ä¸ªå®Œç¾çš„åˆ†æ•° 1.0ï¼Œå› ä¸º candidate ä¸ reference æ˜¯å®Œå…¨åŒ¹é…çš„ã€‚

#### è¯­æ–™åº“çš„BLEUå€¼

NLTK è¿˜æä¾›äº†ä¸€ä¸ª`corpus_bleu()`çš„æ–¹æ³•ï¼Œç”¨äºè®¡ç®—å¤šä¸ªå¥å­ï¼ˆæ®µè½æˆ–è€…æ–‡æ¡£ï¼‰çš„ BLEU çš„å€¼ã€‚

å‚è€ƒå¥å¿…é¡»ç”±ä¸€ç³»åˆ—çš„ documents ç»„æˆï¼Œæ¯ä¸ª document éƒ½åº”è¯¥ç”±ä¸€ç³»åˆ—çš„ references ç»„æˆï¼Œè€Œæ¯ä¸ªå¥å­åˆåº”è¯¥æ‹†åˆ†ä¸ºä¸€ä¸ªä¸ªçš„ tokenã€‚ä¾‹å­ï¼š

```python
from nltk.translate.bleu_score import corpus_bleu
references = [[['this', 'is', 'a ', 'test'], ['this', 'is', 'test']]]
candidates = [['this', 'is', 'a', 'test']]
score = corpus_bleu(references, candidates)
print(score)
```

è¾“å‡ºç»“æœä»ç„¶æ˜¯å®Œç¾åŒ¹é… 1.0ã€‚è¿™ä¸ªä¾‹å­çœ‹èµ·æ¥å’Œä¸Šä¸€ä¸ªå¾ˆç›¸ä¼¼ï¼Œä¸åŒçš„æ˜¯åˆå¤šäº†ä¸€ä¸ªç»´åº¦ï¼ˆä¸‰ä¸ªä¸­æ‹¬å·ï¼‰ã€‚

### Cumulative and Individual BLEU Scores

NLTK ä¸­çš„ BLEU å€¼çš„è®¡ç®—å¯ä»¥æŒ‡å®šä¸åŒçš„ n-gram çš„æƒé‡ã€‚è¿™æ ·å°±æ„å‘³ç€å¯ä»¥çµæ´»ä½¿ç”¨ä¸åŒç±»å‹çš„ BLEU åˆ†æ•°ï¼Œä¾‹å¦‚å•ä¸ªçš„å’Œç´¯è®¡çš„ n-gram åˆ†æ•°ã€‚

#### å•ä¸ªçš„å€¼

å•ä¸ªçš„ n-gram çš„å€¼æ˜¯å¯¹ gram æ˜¯å¦æŒ‰ç‰¹å®šé¡ºåºæ’åˆ—çš„è¯„ä¼°æ–¹å¼ï¼Œæ¯”å¦‚ 1-gram å’Œ 2-gram (biggram)ã€‚ä¾‹å­ï¼š

```python
# 1-gram individual BLEU
from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'small', 'test']]
candidate = ['this', 'is', 'a', 'test']
score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))
print(score)
```

ä¸Šé¢è¿™ä¸ªä¾‹å­æŠŠ 1-gram çš„æƒé‡è®¾ç½®ä¸º 1ï¼Œå…¶ä½™çš„ä¸º 0ã€‚

æ‰€ä»¥è¿è¡Œå‡ºæ¥çš„ç»“æœæ˜¯ 0.75ã€‚

å¦‚æœæŠŠ candidate çš„é¡ºåºæ‰“ä¹±ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
candidate = ['this', 'test', 'is', 'a']
score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))
print(score)
```

è¿è¡Œç»“æœä»ç„¶æ˜¯ 0.75ã€‚

å¦‚æœä½¿ç”¨ n-gramsï¼Œåˆ™ä¾‹å­ (4-grams) å¦‚ä¸‹ï¼š

```python
# n-gram individual BLEU
from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'a', 'test']]
candidate = ['this', 'is', 'a', 'test']
print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))
```

ç»“æœå¦‚ä¸‹ï¼š

```python
Individual 1-gram: 1.000000
Individual 2-gram: 1.000000
Individual 3-gram: 1.000000
Individual 4-gram: 1.000000
```

å°½ç®¡å¯ä»¥è®¡ç®—å•ä¸ªçš„ BLEU çš„å€¼ï¼Œä½†æ˜¯è¿™ç§æ–¹å¼çš„å«ä¹‰æ˜¯æœ‰é™çš„ï¼Œä¸æ˜¯å¸¸è§„ä½¿ç”¨çš„æ–¹å¼ï¼Œæ‰€ä»¥å¼•å‡ºç´¯è®¡çš„ BLEU çš„å€¼ã€‚

#### ç´¯è®¡çš„å€¼

ç´¯è®¡çš„ n-gram æ˜¯æŒ‡è®¡ç®—ä» 1 åˆ° n çš„æ‰€æœ‰é˜¶æ•°çš„å•ä¸ª n-gram çš„å€¼ï¼Œå¹¶é€šè¿‡è®¡ç®—å…¶åŠ æƒå‡ ä½•å¹³å‡å€¼å¯¹å…¶è¿›è¡ŒåŠ æƒã€‚

åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œ`sentence_bleu()`å’Œ`corpus_bleu()`éƒ½æ˜¯è®¡ç®—ç´¯è®¡çš„ 4-gram çš„ BLEU çš„å€¼ï¼Œç§°ä¹‹ä¸º **BELU-4**ã€‚

é€šè¿‡è®¡ç®— BELU-1ï¼ŒBLEU-2ï¼ŒBLEU-3ï¼ŒBLEU-4 çš„ç´¯è®¡å€¼æ¥è¯´æ˜ï¼š

```python
# cumulative BLEU scores
from nltk.translate.bleu_score import sentence_bleu
reference = [['this', 'is', 'small', 'test']]
candidate = ['this', 'is', 'a', 'test']
print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))
print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))
print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))
```

å¾—åˆ°çš„ç»“æœä¸å•ä¸ªçš„å€¼ç›¸æ¯”æœ‰å¾ˆå¤§çš„ä¸åŒï¼š

```python
Cumulative 1-gram: 0.750000
Cumulative 2-gram: 0.500000
Cumulative 3-gram: 0.000000
Cumulative 4-gram: 0.000000
```

### Reference

1. [A Gentle Introduction to Calculating the BELU Score for Text in Python](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)